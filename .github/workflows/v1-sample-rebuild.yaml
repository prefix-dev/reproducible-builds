name: V1 Recipe Sampling

on:
  workflow_dispatch:
    inputs:
      sample_size:
        description: "Number of V1 packages to sample"
        required: true
        default: "20"
      subdir:
        description: "Conda subdir to sample from (noarch, linux-64, osx-arm64, osx-64, win-64)"
        required: false
        default: "noarch"
      seed:
        description: "Random seed for reproducible sampling (optional)"
        required: false
      max_age_days:
        description: "Maximum age of packages in days"
        required: false
        default: "10"

  schedule:
    # Run daily at 3 AM UTC
    - cron: '0 3 * * *'

# Permissions for GitHub Pages deployment and releases
permissions:
  contents: write
  pages: write
  id-token: write

env:
  RATTLER_BUILD_ENABLE_GITHUB_INTEGRATION: true
  REPRO_DB_NAME: repro.db
  REPRO_DOCS_DIR: docs
  PYTHONUTF8: 1

jobs:
  generate-v1-packages:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.generate-matrix.outputs.matrix }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - uses: prefix-dev/setup-pixi@v0.8.1
        with:
          pixi-version: "latest"
          cache: true

      - name: Generate V1 package matrix
        id: generate-matrix
        run: |
          SAMPLE_SIZE="${{ github.event.inputs.sample_size || '20' }}"
          SUBDIR="${{ github.event.inputs.subdir || 'noarch' }}"
          SEED="${{ github.event.inputs.seed }}"
          MAX_AGE="${{ github.event.inputs.max_age_days || '10' }}"

          SEED_ARG=""
          if [ -n "$SEED" ]; then
            SEED_ARG="--seed $SEED"
          fi

          # Generate matrix with full package info
          # The generate-matrix command outputs JSON to stdout, status to stderr
          # shellcheck disable=SC2086
          MATRIX_JSON=$(pixi run repror v1 generate-matrix \
            --size "$SAMPLE_SIZE" \
            --subdir "$SUBDIR" \
            --max-age-days "$MAX_AGE" \
            $SEED_ARG)

          echo "matrix=$MATRIX_JSON" >> "$GITHUB_OUTPUT"
          echo "Generated matrix with $(echo "$MATRIX_JSON" | jq length) packages"
        env:
          LOG_LEVEL: WARNING

  rebuild-v1-packages:
    needs: generate-v1-packages
    if: ${{ needs.generate-v1-packages.outputs.matrix != '[]' }}
    name: ${{ matrix.package.subdir }}/${{ matrix.package.name }}-${{ matrix.package.version }}-${{ matrix.package.build }}
    continue-on-error: true
    strategy:
      fail-fast: false
      matrix:
        package: ${{ fromJson(needs.generate-v1-packages.outputs.matrix) }}

    # Select runner based on package subdir
    runs-on: ${{ (matrix.package.subdir == 'osx-arm64' && 'macos-14') || (matrix.package.subdir == 'osx-64' && 'macos-13') || (matrix.package.subdir == 'win-64' && 'windows-latest') || 'ubuntu-latest' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - uses: prefix-dev/setup-pixi@v0.8.1
        with:
          pixi-version: "latest"
          cache: true

      - name: Create conda-forge build directory structure
        run: |
          # rattler-build rebuild tries to use the original build paths from conda-forge
          # We need to create these directories with write permissions
          sudo mkdir -p /home/conda/feedstock_root/build_artifacts
          sudo chown -R "$USER:$USER" /home/conda
        if: runner.os == 'Linux'

      - name: Rebuild V1 package
        run: |
          # Pass package info as JSON directly - no need to re-fetch repodata
          PACKAGE_INFO='${{ toJson(matrix.package) }}'
          pixi run repror v1 rebuild-one \
            --package-info "$PACKAGE_INFO" \
            --actions-url "https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}" \
            --patch
        env:
          LOG_LEVEL: WARNING

      - name: Upload patch artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: v1-patch-${{ matrix.package.subdir }}-${{ matrix.package.name }}-${{ matrix.package.version }}-${{ matrix.package.build }}
          path: build_info/v1/
          if-no-files-found: ignore

      - name: Upload diff artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: v1-diff-${{ matrix.package.subdir }}-${{ matrix.package.name }}-${{ matrix.package.version }}-${{ matrix.package.build }}
          path: build_info/v1/diffs/
          if-no-files-found: ignore

  patch-v1-db:
    runs-on: ubuntu-latest
    needs: rebuild-v1-packages
    if: always()

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - uses: prefix-dev/setup-pixi@v0.8.1
        with:
          pixi-version: "latest"
          cache: true

      - name: Download all patch artifacts
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          pattern: v1-patch-*
          path: build_info/v1/
          merge-multiple: true

      - name: List patches
        continue-on-error: true
        run: |
          echo "=== V1 Patches ==="
          find build_info/v1 -name "*.json" 2>/dev/null || echo "No patches found"

      - name: Download existing database from release
        continue-on-error: true
        run: |
          gh release download database --pattern 'repro.db' --clobber || echo "No existing database release found"
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Migrate database schema
        run: |
          # Add new columns if they don't exist (for backwards compatibility)
          if [ -f "repro.db" ]; then
            sqlite3 repro.db "ALTER TABLE v1_rebuild ADD COLUMN subdir TEXT;" 2>/dev/null || echo "subdir column already exists"
            sqlite3 repro.db "ALTER TABLE v1_rebuild ADD COLUMN build_string TEXT;" 2>/dev/null || echo "build_string column already exists"
          fi

      - name: Backfill missing subdir and build_string
        run: |
          if [ -f "repro.db" ]; then
            pixi run python << 'EOF'
          import sqlite3
          import re

          conn = sqlite3.connect('repro.db')
          cursor = conn.cursor()

          # Find records with NULL subdir or build_string
          cursor.execute("""
              SELECT id, original_url FROM v1_rebuild
              WHERE subdir IS NULL OR build_string IS NULL
          """)
          rows = cursor.fetchall()

          if not rows:
              print("No records to backfill")
          else:
              print(f"Backfilling {len(rows)} records...")

          for row_id, url in rows:
              # URL format: https://conda.anaconda.org/conda-forge/{subdir}/{name}-{version}-{build}.conda
              # Extract subdir from path segment before filename
              match = re.search(r'/([^/]+)/([^/]+)\.conda$', url)
              if match:
                  subdir = match.group(1)
                  filename = match.group(2)
                  # Extract build string: filename is {name}-{version}-{build}
                  # Build string is everything after the last hyphen before version pattern
                  parts = filename.rsplit('-', 2)
                  if len(parts) >= 3:
                      build_string = parts[2]
                  else:
                      build_string = None

                  cursor.execute("""
                      UPDATE v1_rebuild SET subdir = ?, build_string = ? WHERE id = ?
                  """, (subdir, build_string, row_id))

          conn.commit()
          print(f"Backfilled {len(rows)} records")
          conn.close()
          EOF
          fi

      - name: Merge V1 patches to database
        run: |
          pixi run python << 'EOF'
          import sys
          sys.path.insert(0, 'src')

          from repror.internals.db import setup_engine
          from repror.internals.patch_database import patch_v1_rebuilds_to_db

          setup_engine()
          count = patch_v1_rebuilds_to_db("build_info/v1")
          print(f"Merged {count} V1 rebuild patches")
          EOF

      - name: Upload database to release
        if: github.ref == 'refs/heads/main'
        run: |
          # Create release if it doesn't exist
          gh release view database || gh release create database --title "Database" --notes "Auto-updated reproducibility database"
          # Upload/update the database file
          gh release upload database repro.db --clobber
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

  generate-html:
    runs-on: ubuntu-latest
    needs: patch-v1-db
    if: always()

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - uses: prefix-dev/setup-pixi@v0.8.1
        with:
          pixi-version: "latest"
          cache: true

      - name: Download database from release
        continue-on-error: true
        run: |
          gh release download database --pattern 'repro.db' --clobber || echo "No database release found"
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Migrate database schema
        run: |
          if [ -f "repro.db" ]; then
            sqlite3 repro.db "ALTER TABLE v1_rebuild ADD COLUMN subdir TEXT;" 2>/dev/null || true
            sqlite3 repro.db "ALTER TABLE v1_rebuild ADD COLUMN build_string TEXT;" 2>/dev/null || true
          fi

      - name: Download all diff artifacts
        uses: actions/download-artifact@v4
        continue-on-error: true
        with:
          pattern: v1-diff-*
          path: diffs_staging/
          merge-multiple: true

      - name: Upload diffs to R2
        if: github.ref == 'refs/heads/main'
        run: |
          if [ -d "diffs_staging" ]; then
            echo "=== Uploading diffs to R2 ==="
            # Sync diffs to R2, preserving subdir structure
            aws s3 sync diffs_staging/ s3://reproducible-builds-diffs/ \
              --endpoint-url https://e1a7cde76f1780ec06bac859036dbaf7.r2.cloudflarestorage.com \
              --exclude "*" --include "*.html"
            echo "Done uploading diffs to R2"
          else
            echo "No diffs to upload"
          fi
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.R2_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.R2_SECRET_ACCESS_KEY }}

      - name: Generate HTML pages
        run: |
          pixi run repror generate-html

      - name: Upload Pages artifact
        if: github.ref == 'refs/heads/main'
        uses: actions/upload-pages-artifact@v3
        with:
          path: docs/

  deploy-pages:
    runs-on: ubuntu-latest
    needs: generate-html
    if: github.ref == 'refs/heads/main'

    permissions:
      pages: write
      id-token: write

    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4

  summarize-results:
    runs-on: ubuntu-latest
    needs: [generate-html, deploy-pages]
    if: always()
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - uses: prefix-dev/setup-pixi@v0.8.1
        with:
          pixi-version: "latest"
          cache: true

      - name: Download database from release
        continue-on-error: true
        run: |
          gh release download database --pattern 'repro.db' --clobber || echo "No database release found"
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Migrate database schema
        run: |
          if [ -f "repro.db" ]; then
            sqlite3 repro.db "ALTER TABLE v1_rebuild ADD COLUMN subdir TEXT;" 2>/dev/null || true
            sqlite3 repro.db "ALTER TABLE v1_rebuild ADD COLUMN build_string TEXT;" 2>/dev/null || true
          fi

      - name: Generate summary
        run: |
          pixi run python << 'EOF' >> "$GITHUB_STEP_SUMMARY"
          import sys
          sys.path.insert(0, 'src')

          from repror.internals.db import setup_engine, get_session, V1Rebuild, BuildState
          from sqlmodel import select, func

          setup_engine()

          print("## V1 Recipe Rebuild Results\n")

          with get_session() as session:
              total = session.exec(select(func.count(V1Rebuild.id))).one()
              successful = session.exec(
                  select(func.count(V1Rebuild.id)).where(V1Rebuild.state == BuildState.SUCCESS)
              ).one()
              reproducible = session.exec(
                  select(func.count(V1Rebuild.id)).where(
                      V1Rebuild.state == BuildState.SUCCESS,
                      V1Rebuild.original_hash == V1Rebuild.rebuild_hash
                  )
              ).one()

              print(f"| Metric | Count |")
              print(f"|--------|-------|")
              print(f"| Total V1 rebuilds | {total} |")
              print(f"| Successful rebuilds | {successful} |")
              print(f"| Reproducible | {reproducible} |")

              if successful > 0:
                  rate = 100 * reproducible / successful
                  print(f"| Reproducibility rate | {rate:.1f}% |")

              print("\n### View Results")
              print("- [Recipe Builds Dashboard](https://prefix-dev.github.io/reproducible-builds/)")
              print("- [Conda-Forge Rebuilds](https://prefix-dev.github.io/reproducible-builds/v1.html)")
          EOF
